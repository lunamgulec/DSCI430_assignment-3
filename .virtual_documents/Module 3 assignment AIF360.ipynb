





# May require installation of additional libraries like aif360 and eli5

import pandas as pd
import numpy as np
import joblib

# Fairness metrics
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.metrics import ClassificationMetric

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    make_scorer,
    ConfusionMatrixDisplay,
    accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix
)

import matplotlib.pyplot as plt
import eli5
from sklearn.inspection import permutation_importance





train_df = pd.read_csv("training_set.csv")
test_df = pd.read_csv("testing_set.csv")


test_df.head()


# Separating features and target
X_train, y_train = (
    train_df.drop(columns=["Recidivism_Within_3years"]),
    train_df["Recidivism_Within_3years"],
)
X_test, y_test = (
    test_df.drop(columns=["Recidivism_Within_3years"]),
    test_df["Recidivism_Within_3years"],
)


# Loading classifiers
logreg_model  = joblib.load("models_for_A3/NIJ_logreg.joblib")
rf_model      = joblib.load("models_for_A3/NIJ_rf.joblib")
tree_model    = joblib.load("models_for_A3/NIJ_tree.joblib")
xgboost_model = joblib.load("models_for_A3/NIJ_xgboost.joblib")





results_dict = {}





from aif360.datasets import BinaryLabelDataset
from aif360.metrics import ClassificationMetric
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# When using aif360, the target has to be numerical, so we will start by converting our target column. Since we are 
# only interested in the metrics from the test set, we will convert only the test target

# Define your mapping
label_map = {"No": 0, "Yes": 1}

# Convert y_test
y_test_num = y_test.map(label_map)

# Since aif360 focuses on fairness, it needs us to identify the protected attribute we will use to segment the population.
protected_attr = 'pipeline-2__Race_WHITE'   

# aif360 can not handle Nans - we must impute before converting. For this purpose, I can use the transformer included in 
# the RF model
X_test_array = rf_model.named_steps['ct'].transform(X_test)

feature_names = rf_model.named_steps['ct'].get_feature_names_out()

X_test_transformed = pd.DataFrame(X_test_array, columns=feature_names, index=X_test.index)

# Locate protected attribute column in transformed test set.
protected_idx = X_test_transformed.columns.get_loc(protected_attr)

# Now that we have collected all this information, we can create a BinaryLabelDataset,
# aif360 base class for all structured datasets with binary labels - like ours!
# https://aif360.readthedocs.io/en/stable/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset

aif_test = BinaryLabelDataset(
    favorable_label=1, # Being identified as not having committed a crime is the favorable outcome
    unfavorable_label=0,
    df=pd.concat([X_test_transformed, y_test_num], axis=1),
    label_names=[y_test.name],
    protected_attribute_names=[protected_attr]
)



# Now, we need to do a similar process and create a BinaryLabelDataset for the predictions of the Random Forest model

# First, we'll get the predictions and convert them to 0 and 1
y_pred = rf_model.predict(X_test)
y_pred_num = pd.Series(y_pred, index=y_test.index).map(label_map)

# Then, we will create a copy of the transformed test feature matrix and attach the predictions to it
pred_df = X_test_transformed.copy()
pred_df[y_test.name] = y_pred_num

# Finally, we piece everything together in a BinaryLabelDataset 
aif_pred = BinaryLabelDataset(
    favorable_label=1,
    unfavorable_label=0,
    df=pred_df,
    label_names=[y_test.name],
    protected_attribute_names=[protected_attr]
)


# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)
classified_metric_rf = ClassificationMetric(
    aif_test,
    aif_pred,
    unprivileged_groups=[{protected_attr: 0}],
    privileged_groups=[{protected_attr: 1}]
)

precision = classified_metric_rf.precision()
recall = classified_metric_rf.recall()

# aif360 does not have a function foro F1 - we will compute F1 manually
if (precision + recall) > 0:
    f1 = 2 * (precision * recall) / (precision + recall)
else:
    f1 = 0.0

# Saving results in the dictionary
results_dict["RandomForest"] = {
    "accuracy": format(classified_metric_rf.accuracy(), ".3f"),
    "precision": format(precision, ".3f"),
    "recall": format(recall, ".3f"),
    "f1_score": format(f1, ".3f"),
}



# We can check our results
pd.DataFrame(results_dict)





























# Use confusion_matrix to plot your result here


# As a sanity check, it is a good idea to compare these results with the ones we would get using aif360
# If they don't match, something went wrong
classified_metric_lr.binary_confusion_matrix(privileged=None)





# Use confusion_matrix to plot your result here


classified_metric_rf.binary_confusion_matrix(privileged=None)





# Use confusion_matrix to plot your result here


classified_metric_dt.binary_confusion_matrix(privileged=None)





# Use confusion_matrix to plot your result here


classified_metric_xgb.binary_confusion_matrix(privileged=None)



























































feature_names = np.array(logreg_model.named_steps['columntransformer'].get_feature_names_out())
coeffs = logreg_model.named_steps["logisticregression"].coef_.flatten()
coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=["Coefficient"])
coeff_df_sorted = coeff_df.sort_values(by="Coefficient", ascending=False)


coeff_df_sorted.head(10)


coeff_df_sorted.tail(10)





hard_sample = X_test[5:6]





logreg_model.predict(hard_sample)


# The label for this sample is, in fact, 'Yes'
y_test[5]











from sklearn import tree

tree.plot_tree(tree_model["dt"],fontsize=10)
plt.figure(figsize=(10,6))
plt.show()








from sklearn.tree import export_text
tree_rules = export_text(tree_model.named_steps['dt'], feature_names=list(tree_model.named_steps['ct'].get_feature_names_out()))
print(tree_rules)








import seaborn as sns

feature_importances = tree_model.named_steps["dt"].feature_importances_

# Sort the feature importances from greatest to least using the sorted indices
sorted_indices = feature_importances.argsort()[::-1]
sorted_feature_names = tree_model.named_steps['ct'].get_feature_names_out()[sorted_indices[0:20]] # Output limited to top 20 features
sorted_importances = feature_importances[sorted_indices[0:20]]

# # Create a bar plot of the feature importances
sns.set(rc={'figure.figsize':(11.7,30)})
sns.barplot(x=sorted_importances, y=sorted_feature_names)











tree_model.predict(hard_sample)











# Step 1: create logistic regressor object.
# For simplicity, we will use the already existing "NIJ_logreg.joblib" and re-train it, instead of creating
# a new one. The reason for this decision is that NIJ_logreg.joblib already knows how to handle the features
# of this dataset, while a new one will need to be designed to do so.

surrogate_model_rf = joblib.load("models_for_A3/NIJ_logreg.joblib")

# Step 2: train model on random forest predictions on the training set
y_pred_rf_train = rf_model.predict(X_train)
surrogate_model_rf.fit(X_train, y_pred_rf_train)

# Step 3: visualize weights of surrogate model, as we did for the original logistic regression model
eli5.explain_weights(surrogate_model_rf[-1], feature_names=surrogate_model_rf[:-1].get_feature_names_out())











from sklearn.metrics import r2_score
surrogate_pred = surrogate_model_rf.predict(X_train)

# compute count R2 

















# Use permutation_importance on the random forest model, and save the result in a variable called "out"
out = permutation_importance(rf_model, X_test, y_test, random_state=0)





result = pd.DataFrame({"Name": X_test.columns, "Importance": out["importances_mean"], "STD": out["importances_std"]})
result = result.sort_values(by=['Importance'], ascending=False)

sns.set(rc={'figure.figsize':(11.7,7)})
sns.barplot(data=result[:5], y="Name", x="Importance")








# Your solution here











import shap # may require to downgrade numpy to version = 1.23
shap.initjs()





X_train_enc = pd.DataFrame(
    data=rf_model.named_steps['ct'].transform(X_train),
    columns=feature_names,
    index=X_train.index,
)

X_test_enc = pd.DataFrame(
    data=rf_model.named_steps['ct'].transform(X_test),
    columns=feature_names,
    index=X_test.index,
)

ind = np.random.choice(len(X_test_enc) - 1, 1000)  
# This line just gives 1000 random indexes from the training set
# We do this because getting SHAP values for all samples would be a bit too long, but you 
# are free to try it out!

ind = np.append(ind, 5) # adding the hard sample - we'll need this later





rf_explainer = shap.TreeExplainer(rf_model[-1])  # creating SHAP Explainer based on the model

rf_shap_values = rf_explainer.shap_values(X_test_enc.iloc[ind])  # explaining predictions for 1000 random samples





rf_shap_values[:,:,1]





values = np.abs(rf_shap_values[:,:,1]).mean(0)
pd.DataFrame(data=values, index=feature_names, columns=["SHAP"]).sort_values(
    by="SHAP", ascending=False
)[:10]





shap_figure = shap.summary_plot(rf_shap_values[:, :, 1], X_test_enc.iloc[ind], plot_size=[12,6])








# Your solution here





rf_model.predict(hard_sample)





shap.force_plot(
    rf_explainer.expected_value[1],
    rf_shap_values[:,:,1][-1],
    X_test_enc.iloc[ind[-1]],
    matplotlib=True,
)








# Your answer here




























