{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee102f61",
   "metadata": {},
   "source": [
    "# Module 3 - Algorithm auditing: Accuracy, Fairness and Interpretability \n",
    "\n",
    "\n",
    "### Assignment overview\n",
    "\n",
    "In this assignment, you will be asked to evaluate a set of trained classifiers for accuracy, fairness and transparency. The classifiers have been trained on the [NIJ Recidivism Challenge Dataset](https://nij.ojp.gov/funding/recidivism-forecasting-challenge) to predict whether or not an individual will be arrested for a new crime within 3 years after being released on parole. \n",
    "\n",
    "The assignment is modeled after “Accuracy, Fairness, and Interpretability of Machine Learning Criminal Recidivism Models, by Eric Ingram, Furkan Gursoy, Ioannis A. Kakadiaris (https://arxiv.org/abs/2209.14237). \n",
    "\n",
    "For this assignment, it is possible to work in **groups of up to 2 students**. \n",
    "\n",
    "### Group members\n",
    "Leave blanks if group has less than 2 members:\n",
    "- Student 1: Luna Gulec \n",
    "- Student 2: Athena Wong\n",
    "\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "1. Describe different fairness metrics, such as statistical parity, equal opportunity and equal accuracy \n",
    "2. Discuss fairness and fairness metrics from the perspective of multiple stakeholders \n",
    "3. Define objective functions based on fairness metrics  \n",
    "4. Evaluate a model’s transparency using strategies such as global surrogate models, permutation feature importance, and Shapley Additive Explanations (SHAP) \n",
    "5. Evaluate common machine learning models based on their accuracy, fairness and interpretability \n",
    "6. Describe how metrics such as accuracy and fairness need to be balanced for a trained model to have acceptable accuracy and low bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96ca9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'aif360[AdversarialDebiasing]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444af5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf5160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e3a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'aif360[Reductions]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca85f2",
   "metadata": {},
   "source": [
    "## Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e949dd59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "# May require installation of additional libraries like aif360 and eli5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import eli5\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3af91b",
   "metadata": {},
   "source": [
    "# Part 1: Getting started:\n",
    "\n",
    "Before starting this assignment, we ask you to read the paper it has been modeled after, to get an idea of the problem we are working on: https://arxiv.org/abs/2209.14237\n",
    "\n",
    "You can also review the original dataset source [here](https://nij.ojp.gov/funding/recidivism-forecasting-challenge). The website includes a lot of information on the dataset and a detailed description of each of its columns (look for Appendix 2: Codebook).\n",
    "\n",
    "Now that you have familiarized with the problem, you know that the goal is predicting the binary variable `Recidivism_Within_3years`, which indicates whether or not the person has committed a new felony or misdemeanour within 3 years from the beginning of parole supervision. \n",
    "\n",
    "The National Institute of Justice’s (NIJ) obviously would want to deploy a highly accurate predictive model, to make sure that only deserving people get released on parole. Unfortunately, the existence of bias in the training set (typically historical or representation bias) makes it very likely to end up with an unfair classifier, that is, a classifier that produces different results for different protected classes of population.\n",
    "\n",
    "Your job is to evaluate 4 classifiers, pre-trained and provided to you. This is called **algorithm auditing:** you are not the designer of the model, but you are in charge of evaluating its performance. Algorithm auditing can focus on various metrics and populations of interest, but in this case we will focus on evaluating **accuracy, fairness and transparency** of each algorithm.\n",
    "\n",
    "To begin, load the dataset and classifiers by running the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1894ff7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/training_set.csv\")\n",
    "test_df = pd.read_csv(\"./data/testing_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cde51bd-5444-4e6a-b7ac-24a4187c9d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>...</th>\n",
       "      <th>DrugTests_Meth_Positive</th>\n",
       "      <th>DrugTests_Other_Positive</th>\n",
       "      <th>Percent_Days_Employed</th>\n",
       "      <th>Jobs_Per_Year</th>\n",
       "      <th>Employment_Exempt</th>\n",
       "      <th>Recidivism_Within_3years</th>\n",
       "      <th>Recidivism_Arrest_Year1</th>\n",
       "      <th>Recidivism_Arrest_Year2</th>\n",
       "      <th>Recidivism_Arrest_Year3</th>\n",
       "      <th>Training_Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3735</td>\n",
       "      <td>3848</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>48 or older</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.299385</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5039</td>\n",
       "      <td>5174</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>48 or older</td>\n",
       "      <td>20</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Specialized</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.544397</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16027</td>\n",
       "      <td>16485</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33-37</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>High</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51049</td>\n",
       "      <td>1.277098</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18863</td>\n",
       "      <td>19486</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>43-47</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.006198</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14013</td>\n",
       "      <td>14408</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33-37</td>\n",
       "      <td>14</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.318592</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     ID Gender   Race Age_at_Release  Residence_PUMA  \\\n",
       "0        3735   3848      M  BLACK    48 or older               1   \n",
       "1        5039   5174      M  BLACK    48 or older              20   \n",
       "2       16027  16485      F  WHITE          33-37              10   \n",
       "3       18863  19486      F  WHITE          43-47              11   \n",
       "4       14013  14408      M  WHITE          33-37              14   \n",
       "\n",
       "  Gang_Affiliated  Supervision_Risk_Score_First Supervision_Level_First  \\\n",
       "0              No                           6.0                Standard   \n",
       "1              No                           2.0             Specialized   \n",
       "2             NaN                           6.0                    High   \n",
       "3             NaN                           5.0                Standard   \n",
       "4              No                           4.0                Standard   \n",
       "\n",
       "       Education_Level  ... DrugTests_Meth_Positive DrugTests_Other_Positive  \\\n",
       "0  High School Diploma  ...                     0.0                      0.0   \n",
       "1  High School Diploma  ...                     0.0                      0.0   \n",
       "2  High School Diploma  ...                     0.0                      0.0   \n",
       "3  High School Diploma  ...                     0.0                      0.0   \n",
       "4  High School Diploma  ...                     0.0                      0.0   \n",
       "\n",
       "  Percent_Days_Employed Jobs_Per_Year Employment_Exempt  \\\n",
       "0               1.00000      0.299385                No   \n",
       "1               1.00000      1.544397                No   \n",
       "2               0.51049      1.277098                No   \n",
       "3               1.00000      1.006198                No   \n",
       "4               1.00000      1.318592                No   \n",
       "\n",
       "  Recidivism_Within_3years Recidivism_Arrest_Year1 Recidivism_Arrest_Year2  \\\n",
       "0                       No                      No                      No   \n",
       "1                      Yes                      No                      No   \n",
       "2                       No                      No                      No   \n",
       "3                       No                      No                      No   \n",
       "4                       No                      No                      No   \n",
       "\n",
       "  Recidivism_Arrest_Year3 Training_Sample  \n",
       "0                      No               0  \n",
       "1                     Yes               1  \n",
       "2                      No               1  \n",
       "3                      No               1  \n",
       "4                      No               1  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2cb1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separating features and target\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"Recidivism_Within_3years\"]),\n",
    "    train_df[\"Recidivism_Within_3years\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"Recidivism_Within_3years\"]),\n",
    "    test_df[\"Recidivism_Within_3years\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84edeb61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsci430/lib/python3.10/pickle.py:1718: UserWarning: [20:24:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1767713854601/work/src/gbm/../common/error_msg.h:83: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  setstate(state)\n"
     ]
    }
   ],
   "source": [
    "# Loading classifiers\n",
    "logreg_model  = joblib.load(\"./models_for_A3/NIJ_logreg.joblib\")\n",
    "rf_model      = joblib.load(\"./models_for_A3/NIJ_rf.joblib\")\n",
    "tree_model    = joblib.load(\"./models_for_A3/NIJ_tree.joblib\")\n",
    "xgboost_model = joblib.load(\"./models_for_A3/NIJ_xgboost.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecefb9",
   "metadata": {},
   "source": [
    "# Part 2: Classifiers' Accuracy (and other performance metrics):\n",
    "\n",
    "First, we will evaluate each classifier's accuracy, together with other performance metrics that help us understanding how reliable the classifier's answers are. In addition to accuracy, we will use, **precision, recall and F1 score.**\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Can you provide definition and formula for accuracy, precision, recall and F1 score, as a function of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN)?\n",
    "\n",
    "It may help you use this table for reference:\n",
    "\n",
    "<img src=\"ConfMatrix.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Here, we are giving you the definition of AUC, as a reminder and example (note that the other metrics will need the formula):\n",
    "\n",
    "**AUC:** AUC stands for Area Under the ROC curve. The ROC (receiver operating characteristic) curve is a plot of the recall and false positive rate of a classifier for different classification thresholds (see [here](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for more details). AUC values go between 0 and 1. Higher values are more desirable as they indicate that the classifier is good at avoiding both false positives and false negatives. A value of 0.5 for a binary classification indicates that the classifier is no better at predicting the outcome than random guessing.\n",
    "\n",
    "**Accuracy:** Accuracy is the overall correctness. **Accuracy = (TP+TN)/total**\n",
    "\n",
    "**Recall:** Recall measures how often a machine learning model correctly identifies positive instances (true positives) from all the actual positive samples in the dataset. **Recall = TP/(TP+FN)**\n",
    "\n",
    "**Precision:** Precision measures how often a machine learning model correctly predicts the positive class. **Precision = TP/(TP+FP)** \n",
    "\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "For every classifier given, calculate and report accuracy, precision, recall and F1 score on the test set. For ease of visualization and comparison, we will summarize these results in this dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58e49a1-3aa7-40bb-bcb4-909681672ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0848-2333-4d26-94c3-46eec54d4216",
   "metadata": {},
   "source": [
    "There are different options available to us to compute fairness metrics, such as the many functions included in sklearn. For this exercise, however, we will focus on working with the [aif360](https://aif360.readthedocs.io/en/stable/Getting%20Started.html) library, an open source library designed specifically to examine and mitigate discrimination and bias in machine learning models.\n",
    "\n",
    "Let's see in an example how we can use aif360 to compute standard performance metrics. We will start with the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa2248c-2b98-477a-8ae9-c512c3ff00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# When using aif360, the target has to be numerical, so we will start by converting our target column. Since we are \n",
    "# only interested in the metrics from the test set, we will convert only the test target\n",
    "\n",
    "# Define your mapping\n",
    "label_map = {\"No\": 0, \"Yes\": 1}\n",
    "\n",
    "# Convert y_test\n",
    "y_test_num = y_test.map(label_map)\n",
    "\n",
    "# Since aif360 focuses on fairness, it needs us to identify the protected attribute we will use to segment the population.\n",
    "protected_attr = 'pipeline-2__Race_WHITE'   \n",
    "\n",
    "# aif360 can not handle Nans - we must impute before converting. For this purpose, I can use the transformer included in \n",
    "# the RF model\n",
    "X_test_array = rf_model.named_steps['ct'].transform(X_test)\n",
    "\n",
    "feature_names = rf_model.named_steps['ct'].get_feature_names_out()\n",
    "\n",
    "X_test_transformed = pd.DataFrame(X_test_array, columns=feature_names, index=X_test.index)\n",
    "\n",
    "# Locate protected attribute column in transformed test set.\n",
    "protected_idx = X_test_transformed.columns.get_loc(protected_attr)\n",
    "\n",
    "# Now that we have collected all this information, we can create a BinaryLabelDataset,\n",
    "# aif360 base class for all structured datasets with binary labels - like ours!\n",
    "# https://aif360.readthedocs.io/en/stable/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset\n",
    "\n",
    "aif_test = BinaryLabelDataset(\n",
    "    favorable_label=1, # Being identified as not having committed a crime is the favorable outcome\n",
    "    unfavorable_label=0,\n",
    "    df=pd.concat([X_test_transformed, y_test_num], axis=1),\n",
    "    label_names=[y_test.name],\n",
    "    protected_attribute_names=[protected_attr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914bcf52-a82a-49fb-b64b-3621bfd5ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to do a similar process and create a BinaryLabelDataset for the predictions of the Random Forest model\n",
    "\n",
    "# First, we'll get the predictions and convert them to 0 and 1\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_num = pd.Series(y_pred, index=y_test.index).map(label_map)\n",
    "\n",
    "# Then, we will create a copy of the transformed test feature matrix and attach the predictions to it\n",
    "pred_df = X_test_transformed.copy()\n",
    "pred_df[y_test.name] = y_pred_num\n",
    "\n",
    "# Finally, we piece everything together in a BinaryLabelDataset \n",
    "aif_pred = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=pred_df,\n",
    "    label_names=[y_test.name],\n",
    "    protected_attribute_names=[protected_attr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7acd2532-1338-4cb5-a72a-aed7b822d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)\n",
    "classified_metric_rf = ClassificationMetric(\n",
    "    aif_test,\n",
    "    aif_pred,\n",
    "    unprivileged_groups=[{protected_attr: 0}],\n",
    "    privileged_groups=[{protected_attr: 1}]\n",
    ")\n",
    "\n",
    "precision = classified_metric_rf.precision()\n",
    "recall = classified_metric_rf.recall()\n",
    "\n",
    "# aif360 does not have a function foro F1 - we will compute F1 manually\n",
    "if (precision + recall) > 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "# Saving results in the dictionary\n",
    "results_dict[\"RandomForest\"] = {\n",
    "    \"accuracy\": format(classified_metric_rf.accuracy(), \".3f\"),\n",
    "    \"precision\": format(precision, \".3f\"),\n",
    "    \"recall\": format(recall, \".3f\"),\n",
    "    \"f1_score\": format(f1, \".3f\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9551a8-03c3-4999-858b-1897a22d185e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RandomForest\n",
       "accuracy         0.720\n",
       "precision        0.740\n",
       "recall           0.788\n",
       "f1_score         0.763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check our results\n",
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f88565-86d0-4213-a105-ec2311b821d4",
   "metadata": {},
   "source": [
    "Repeat the steps above for the logistic regression, decision tree, and xgboost model, and add the results to the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4a652",
   "metadata": {},
   "source": [
    "### LogReg Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0b26ed-b6b7-4b56-bb30-2cf856c80f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the above 3 cells, editing for the predictions of the Logistic Regression model\n",
    "\n",
    "# First, we'll get the predictions and convert them to 0 and 1\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "y_pred_num = pd.Series(y_pred, index=y_test.index).map(label_map)\n",
    "\n",
    "# Then, we will create a copy of the transformed test feature matrix and attach the predictions to it\n",
    "pred_df = X_test_transformed.copy()\n",
    "pred_df[y_test.name] = y_pred_num\n",
    "\n",
    "# Finally, we piece everything together in a BinaryLabelDataset \n",
    "aif_pred = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=pred_df,\n",
    "    label_names=[y_test.name],\n",
    "    protected_attribute_names=[protected_attr]\n",
    ")\n",
    "\n",
    "# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)\n",
    "classified_metric_lr = ClassificationMetric(\n",
    "    aif_test,\n",
    "    aif_pred,\n",
    "    unprivileged_groups=[{protected_attr: 0}],\n",
    "    privileged_groups=[{protected_attr: 1}]\n",
    ")\n",
    "\n",
    "precision = classified_metric_lr.precision()\n",
    "recall = classified_metric_lr.recall()\n",
    "\n",
    "# compute F1 manually\n",
    "if (precision + recall) > 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "# Saving results in the dictionary\n",
    "results_dict[\"LogisticRegression\"] = {\n",
    "    \"accuracy\": format(classified_metric_lr.accuracy(), \".3f\"),\n",
    "    \"precision\": format(precision, \".3f\"),\n",
    "    \"recall\": format(recall, \".3f\"),\n",
    "    \"f1_score\": format(f1, \".3f\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1078f9",
   "metadata": {},
   "source": [
    "### Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "893765a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the above RF cells, editing for the predictions of the Decision Tree model\n",
    "\n",
    "# First, we'll get the predictions and convert them to 0 and 1\n",
    "y_pred = tree_model.predict(X_test)\n",
    "y_pred_num = pd.Series(y_pred, index=y_test.index).map(label_map)\n",
    "\n",
    "# Then, we will create a copy of the transformed test feature matrix and attach the predictions to it\n",
    "pred_df = X_test_transformed.copy()\n",
    "pred_df[y_test.name] = y_pred_num\n",
    "\n",
    "# Finally, we piece everything together in a BinaryLabelDataset \n",
    "aif_pred = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=pred_df,\n",
    "    label_names=[y_test.name],\n",
    "    protected_attribute_names=[protected_attr]\n",
    ")\n",
    "\n",
    "# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)\n",
    "classified_metric_dt = ClassificationMetric(\n",
    "    aif_test,\n",
    "    aif_pred,\n",
    "    unprivileged_groups=[{protected_attr: 0}],\n",
    "    privileged_groups=[{protected_attr: 1}]\n",
    ")\n",
    "\n",
    "precision = classified_metric_dt.precision()\n",
    "recall = classified_metric_dt.recall()\n",
    "\n",
    "# compute F1 manually\n",
    "if (precision + recall) > 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "# Saving results in the dictionary\n",
    "results_dict[\"DecisionTree\"] = {\n",
    "    \"accuracy\": format(classified_metric_dt.accuracy(), \".3f\"),\n",
    "    \"precision\": format(precision, \".3f\"),\n",
    "    \"recall\": format(recall, \".3f\"),\n",
    "    \"f1_score\": format(f1, \".3f\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c84c8",
   "metadata": {},
   "source": [
    "### XGBoost Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0efac584",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input DataFrames cannot contain NA values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m pred_df[y_test\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m y_pred_num\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Finally, we piece everything together in a BinaryLabelDataset \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m aif_pred \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryLabelDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfavorable_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43munfavorable_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotected_attribute_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprotected_attr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m classified_metric_xgb \u001b[38;5;241m=\u001b[39m ClassificationMetric(\n\u001b[1;32m     22\u001b[0m     aif_test,\n\u001b[1;32m     23\u001b[0m     aif_pred,\n\u001b[1;32m     24\u001b[0m     unprivileged_groups\u001b[38;5;241m=\u001b[39m[{protected_attr: \u001b[38;5;241m0\u001b[39m}],\n\u001b[1;32m     25\u001b[0m     privileged_groups\u001b[38;5;241m=\u001b[39m[{protected_attr: \u001b[38;5;241m1\u001b[39m}]\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dsci430/lib/python3.10/site-packages/aif360/datasets/binary_label_dataset.py:21\u001b[0m, in \u001b[0;36mBinaryLabelDataset.__init__\u001b[0;34m(self, favorable_label, unfavorable_label, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfavorable_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(favorable_label)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfavorable_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(unfavorable_label)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBinaryLabelDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dsci430/lib/python3.10/site-packages/aif360/datasets/structured_dataset.py:94\u001b[0m, in \u001b[0;36mStructuredDataset.__init__\u001b[0;34m(self, df, label_names, protected_attribute_names, instance_weights_name, scores_names, unprivileged_protected_attributes, privileged_protected_attributes, metadata)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide a pandas DataFrame representing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe data (features, labels, protected attributes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput DataFrames cannot contain NA values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "\u001b[0;31mValueError\u001b[0m: Input DataFrames cannot contain NA values."
     ]
    }
   ],
   "source": [
    "# adapted from the above 3 cells, editing for the predictions of the Logistic Regression model\n",
    "\n",
    "# First, we'll get the predictions and convert them to 0 and 1\n",
    "y_pred = xgboost_model.predict(X_test)\n",
    "y_pred_num = pd.Series(y_pred, index=y_test.index).map(label_map)\n",
    "\n",
    "# Then, we will create a copy of the transformed test feature matrix and attach the predictions to it\n",
    "pred_df = X_test_transformed.copy()\n",
    "pred_df[y_test.name] = y_pred_num\n",
    "\n",
    "# Finally, we piece everything together in a BinaryLabelDataset \n",
    "aif_pred = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=pred_df,\n",
    "    label_names=[y_test.name],\n",
    "    protected_attribute_names=[protected_attr]\n",
    ")\n",
    "\n",
    "# We are now ready to compute some metrics using aif_test (our ground truth) and aif_pred (our predictions)\n",
    "classified_metric_xgb = ClassificationMetric(\n",
    "    aif_test,\n",
    "    aif_pred,\n",
    "    unprivileged_groups=[{protected_attr: 0}],\n",
    "    privileged_groups=[{protected_attr: 1}]\n",
    ")\n",
    "\n",
    "precision = classified_metric_xgb.precision()\n",
    "recall = classified_metric_xgb.recall()\n",
    "\n",
    "# compute F1 manually\n",
    "if (precision + recall) > 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "else:\n",
    "    f1 = 0.0\n",
    "\n",
    "# Saving results in the dictionary\n",
    "results_dict[\"XGBoost\"] = {\n",
    "    \"accuracy\": format(classified_metric_xgb.accuracy(), \".3f\"),\n",
    "    \"precision\": format(precision, \".3f\"),\n",
    "    \"recall\": format(recall, \".3f\"),\n",
    "    \"f1_score\": format(f1, \".3f\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92f6d4",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "For every classifier given, plot the confusion matrices on training and test set. Here is another function you will find helpful for this task: [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da28b18",
   "metadata": {},
   "source": [
    "#### Confusion Matrix - logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac7ebd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use confusion_matrix to plot your result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7664d316-4d37-4a6d-a13e-0a928e899af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classified_metric_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# As a sanity check, it is a good idea to compare these results with the ones we would get using aif360\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If they don't match, something went wrong\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclassified_metric_lr\u001b[49m\u001b[38;5;241m.\u001b[39mbinary_confusion_matrix(privileged\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classified_metric_lr' is not defined"
     ]
    }
   ],
   "source": [
    "# As a sanity check, it is a good idea to compare these results with the ones we would get using aif360\n",
    "# If they don't match, something went wrong\n",
    "classified_metric_lr.binary_confusion_matrix(privileged=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27170a82",
   "metadata": {},
   "source": [
    "#### Confusion Matrix - random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ffa2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use confusion_matrix to plot your result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6ad16-8900-4e18-9d13-7aec70757222",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_metric_rf.binary_confusion_matrix(privileged=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899a1c",
   "metadata": {},
   "source": [
    "#### Confusion Matrix - decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f461f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use confusion_matrix to plot your result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f59da-02ea-4f25-b4b1-90e535b1d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_metric_dt.binary_confusion_matrix(privileged=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208741d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use confusion_matrix to plot your result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb87eec-9bce-421e-9f52-1edeb836f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_metric_xgb.binary_confusion_matrix(privileged=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d77281",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Based on the results obtained so far, answer the following questions, providing an explanation and trying to base your decision on multiple metrics:\n",
    "- Which classifiers would you choose for deployment?\n",
    "- Which classifier is the most \"severe\" (a.k.a. classifies more people as at risk of committing another crime within 3 years)?\n",
    "- Which classifier is the most cautious (a.k.a. classifies less people as at risk of committing another crime within 3 years)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e4a24",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a86647",
   "metadata": {},
   "source": [
    "# Part 3 :  Fairness Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2405b3b",
   "metadata": {},
   "source": [
    "Now that we have an understanding of how accurate our classifiers are across all samples, we need to measure their *fairness* across different categories. In similar problems, we are typically concerned with the classifiers being fair across different segments of protected populations (e.g. different genders or ethnicities). The original paper evaluates fairness for both gender and race, but for the purpose of this exercise we will only look at fairness across race, that is, for white and Black defendants.\n",
    "\n",
    "### Question 5\n",
    "\n",
    "As we have seen in class, there is not just one fairness metric, but several, as they have different ways to identify different treatments across populations. The metrics used in the paper are:\n",
    "\n",
    "1. *Predicted Positive Rate Disparity (PPRD)*, whether the numbers of positive predictions are on par across groups.\n",
    "2. *Predicted Positive Group Rate Disparity (PPGRD)*, whether the rates of positive predictions are on par across groups.\n",
    "3. *False Discovery Rate Disparity (FDRD)*, whether the ratios of false positives to predicted positives are on par across groups.\n",
    "4. *False Positive Rate Disparity (FPRD)*, whether the ratios of false positives to actual negatives are on par across groups.\n",
    "5. *False Omission Rate Disparity (FORD)*, whether the ratios of false negatives to predicted negatives are on par across groups.\n",
    "6. *False Negative Rate Disparity (FNRD)*, whether the ratios of false negatives to actual positives are on par across groups.\n",
    "\n",
    "Before jumping into code writing, we must make sure that we have a solid understanding of how these metrics are computed from the True Positive, True Negative, False Positive, and False Negative values *for each group*. We will add the subscript *b* and *w* when appropriate to identify metrics from the group of black or white defendants, respectively. Then, we will write the equations for all fairness metrics. The first one is provided to you as an example:\n",
    "\n",
    "| Metric | Formula |\n",
    "| :------| :---------|\n",
    "| **PPRD** | (TP_b + FP_b) / (TP_w + FP_w) |\n",
    "| **PPGRD** | ((TP_b + FP_b) / (TP_b + FP_b + TN_b + FN_b)) / ((TP_w + FP_w) / (TP_w + FP_w + TN_w + FN_w)) |\n",
    "| **FDRD** | (FP_b / (FP_b + TP_b)) / (FP_w / (FP_w + TP_w)) |\n",
    "| **FPRD** | (FP_b / (FP_b + TN_b)) / (FP_w / (FP_w + TN_w)) |\n",
    "| **FORD** | (FN_b / (FN_b + TN_b)) / (FN_w / (FN_w + TN_w)) |\n",
    "| **FNRD** | (FN_b / (FN_b + TP_b)) / (FN_w / (FN_w + TP_w)) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a20bcc",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Now that we have our formulas, let's test them by manually calculating some fairness metrics based on these two hypothetical confusion matrices:\n",
    "\n",
    "Helping yourself with the table from Question 5, compute the PPGDR, FPRD and FNRD for this scenario, using Group A as the reference (denominator). If we assumed that the positive class (1) represents the most desirable outcome, which group is at an advantage? Explain your answer.\n",
    "<!-- \n",
    "| Confusion Matrix Group A | | | Confusion Matrix Group B |  | |\n",
    "|--------------------|--|--|--------------------|--|--|\n",
    "|        | Pred 1 | Pred 0 |        | Pred 1 | Pred 0 |\n",
    "| **Actual 1** | 55 | 5 | **Actual 1** | 30 | 30 |\n",
    "| **Actual 0** | 30 | 410 | **Actual 0** | 20 | 420 | -->\n",
    "\n",
    "**Confusion Matrix – Group A**\n",
    "\n",
    "| | Pred 1 | Pred 0 |\n",
    "| --- | --- | --- |\n",
    "| Actual 1 | 55 | 5 |\n",
    "| Actual 0 | 30 | 410 |\n",
    "\n",
    "**Confusion Matrix – Group B**\n",
    "\n",
    "|  | Pred 1 | Pred 0 |\n",
    "| --- | --- | --- |\n",
    "| Actual 1 | 30 | 30 |\n",
    "| Actual 0 | 20 | 420 |\n",
    "\n",
    "**PPGRD** = ((TP_b + FP_b) / (TP_b + FP_b + TN_b + FN_b)) / ((TP_w + FP_w) / (TP_w + FP_w + TN_w + FN_w))\\\n",
    "          = ((30 + 20)/(30 + 20 + 420 + 30)) / ((55 + 30)/(55 + 30 + 410 + 5))\\\n",
    "          = (50 / 500) / (85 / 500)\\  \n",
    "          = 0.10 / 0.17  \\\n",
    "          = **0.5882**\n",
    "\n",
    "**FPRD** = (FP_b / (FP_b + TN_b)) / (FP_w / (FP_w + TN_w))\\\n",
    "         = (20/(20 + 420)) / (30/(30 + 410))\\\n",
    "         = (20 / 440) / (30 / 440)  \\\n",
    "         = 0.04545455 / 0.06818182 \\\n",
    "         = **0.6533** \n",
    "\n",
    "**FNRD** = (FN_b / (FN_b + TP_b)) / (FN_w / (FN_w + TP_w))\\\n",
    "         = (30/(30 + 30)) / (5/(5+55))\\\n",
    "         = (30 / 60) / (5 / 60)  \\\n",
    "         = 0.50 / 0.083  \\\n",
    "         = **6.0241**\n",
    "\n",
    "Since class 1 represents the desirable outcome:\n",
    "- PPGRD < 1 -> Group B receives fewer positive predictions\n",
    "- FNRD >> 1 -> Group B is much more likely to be falsely denied a positive outcome\n",
    "\n",
    "**Conclusion:** Group A is advantaged relative to Group B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9b1db-a370-4234-9f23-d4359a079962",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Now we will move to computing a selection of fairness metrics on the 4 estimators we are testing, so that we may test if they are producing fair classifications. We will compute all the metrics, except for PPDR.\n",
    "\n",
    "Since we will be using the `aif360` library, you will need to read its documentation to figure out which functions will give us the result we are looking for. Check the documentation on the [`ClassificationMetric` class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#), and complete the table below with the correct methods. The first one is given as an example:\n",
    "\n",
    "| Metric    | ClassificationMetric method |\n",
    "| :-------- | :------- |\n",
    "| PPGRD |  `disparate_impact()` |\n",
    "| FDRD | `false_discovery_rate_ratio()`  |\n",
    "| FPRD | `false_positive_rate_ratio()`  |\n",
    "| FORD | `false_omission_rate_ratio()`  |\n",
    "| FNRD | `false_negative_rate_ratio()`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2f502",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "We are finally ready to compute the fairness metrics on our models! Do so for all models, and add the results to `results_dict`, so that we may easily compare them later.\n",
    "\n",
    "**Hint:** it is highly recommended to repurpose the `ClassificationMetric` objects created in Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90067b",
   "metadata": {},
   "source": [
    "### LogReg Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227bc0c-bf7a-4502-be43-381a971a412c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea582952",
   "metadata": {},
   "source": [
    "### Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a76436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654954e1",
   "metadata": {},
   "source": [
    "### Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd684c-0010-4fc3-bdd3-8ddcd5ca7c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcef35d3",
   "metadata": {},
   "source": [
    "### XGBoost Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cc84b-4626-447b-82ad-8ec538feb674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2c18009",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Based on the results obtained so far, answer the following questions, providing an explanation for each answer:\n",
    "- Which model would you say exhibits the least amount of bias? Which one shows the most bias?\n",
    "- Based on the application, which fairness metric(s) do you think should be the most important? Which one(s) could be taken less into consideration?\n",
    "- Finally, based on the fairness results, which model would you pick for this application? Is your pick different from the one based on performance metrics alone?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738bfe11",
   "metadata": {},
   "source": [
    "# Part 4: Interpretability Evaluation:\n",
    "\n",
    "Finally, we will evaluate the *interpretability* of our models. It is important to be able to explain how the model uses each feature to make its predictions and *why* a model has given a particular response for an individual - especially important when, like in this case, people's lives are being affected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e8924",
   "metadata": {},
   "source": [
    "### Inherently Interpretable Models\n",
    "\n",
    "Some models are known to be *inherently interpretable*, meaning we can decifer the model behavior by looking at its parameters. These models are also called \"white-box\" models. Logistic regression models and decision trees - in some cases - fall in this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3b47e",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Run the cells below and look at the weights of the logistic regression model. For simplicity, the cells below show the 10 most positive and 10 most negative coefficients. What features bring the prediction more toward the positive class? What other features push the prediction toward the negative class? Do you see any coefficients that may be unfairly influencing the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fac4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = np.array(logreg_model.named_steps['columntransformer'].get_feature_names_out())\n",
    "coeffs = logreg_model.named_steps[\"logisticregression\"].coef_.flatten()\n",
    "coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=[\"Coefficient\"])\n",
    "coeff_df_sorted = coeff_df.sort_values(by=\"Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16e89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coeff_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9627f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coeff_df_sorted.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47590ed",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Now, let's look at a particular sample and try to explain its prediction. We have picked this sample because its feature values make it a hard case, one very close to the threshold between positive and negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79984a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hard_sample = X_test[5:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3122999",
   "metadata": {},
   "source": [
    "If you look at the ground truth for this sample you will see that this person has, in fact, committed a new crime within 3 years from release. But what is the prediction of the logistic regression model? Find the answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6baf318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logreg_model.predict(hard_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e81a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label for this sample is, in fact, 'Yes'\n",
    "y_test[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190d5d7",
   "metadata": {},
   "source": [
    "Take a closer look at the feature values for this sample. What seems to have contributed the most to the final prediction? What feature pushed the most in the opposite direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24c159-84ad-4066-ade7-769f422f4858",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f682f5a",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "We said that decision trees are also inherently interpretable - *potentially*. That is because, in theory, it is possible to look at the tree structure and to follow the path along the tree to see how each node influenced the decision. But this is only possible if the tree has a reasonably small size.\n",
    "\n",
    "Run the cell below and see if you can tell what are the most influencial features in the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8d359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(tree_model[\"dt\"],fontsize=10)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3de87e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b43ae1",
   "metadata": {},
   "source": [
    "If the method above was not satisfactory, you can try visualizing all the rules of the decision tree as text. Is this any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10999cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(tree_model.named_steps['dt'], feature_names=list(tree_model.named_steps['ct'].get_feature_names_out()))\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8f671",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b26a8d46",
   "metadata": {},
   "source": [
    "When it is not possible to interpret a decision tree because of its complex structure, we can still extract other information from it that will help us understand the features' importance in the decision. The code in the cell below extracts the feature importances from the model (line 3), then uses this information to create a bar plot of features sorted by importance. The feature importance extracted this way is based on [Gini Importance](https://www.codecademy.com/article/fe-feature-importance-final) (as it is done in the original paper), which reflects how the features were picked when building the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e891206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "feature_importances = tree_model.named_steps[\"dt\"].feature_importances_\n",
    "\n",
    "# Sort the feature importances from greatest to least using the sorted indices\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "sorted_feature_names = tree_model.named_steps['ct'].get_feature_names_out()[sorted_indices[0:20]] # Output limited to top 20 features\n",
    "sorted_importances = feature_importances[sorted_indices[0:20]]\n",
    "\n",
    "# # Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7,30)})\n",
    "sns.barplot(x=sorted_importances, y=sorted_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc53165",
   "metadata": {},
   "source": [
    "Comment on the features importance of the tree model, compared to those seen in the logistic regression model, as well as the original paper results. Also, **what is a big limitation of using feature importance, compared to observing the coefficient of the logistic regression model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af139a53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365cd708",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "As before, we are interested in evaluating how the model classifies a particular sample. Let's start looking at the classification for our `hard_sample`. Is it correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model.predict(hard_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81084c12",
   "metadata": {},
   "source": [
    "We would like to be able to tell what sequence of rules has led to this final decision, but, for a tree this large, this can be difficult, unless we want to manually sift through the list of rules or write some elaborate custom code. In the next sections, we will see an alternative method (SHAP) to achieve this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7aafa",
   "metadata": {},
   "source": [
    "### Question 11: Evaluation of Non-inherently Interpretable Models Using a Surrogate Model\n",
    "\n",
    "Models that are not inherently interpretable (\"black box\" models) can still be examined to understand how they used the available features to make their predictions. In fact, there are many strategies to do this. The first one we are going to see is through use of a **surrogate model.** In this case, we train another model - an inherently interpretable one, such as a logistic regressor - on the *predictions* of the black box model, and then we try to interpret *its parameters*. Let's complete the code below to do that on the 3 non-inherently interpretable models included in this exercise: Random Forest and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f75190",
   "metadata": {},
   "source": [
    "#### Surrogate for Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2dba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: create logistic regressor object.\n",
    "# For simplicity, we will use the already existing \"NIJ_logreg.joblib\" and re-train it, instead of creating\n",
    "# a new one. The reason for this decision is that NIJ_logreg.joblib already knows how to handle the features\n",
    "# of this dataset, while a new one will need to be designed to do so.\n",
    "\n",
    "surrogate_model_rf = joblib.load(\"models_for_A3/NIJ_logreg.joblib\")\n",
    "\n",
    "# Step 2: train model on random forest predictions on the training set\n",
    "y_pred_rf_train = rf_model.predict(X_train)\n",
    "surrogate_model_rf.fit(X_train, y_pred_rf_train)\n",
    "\n",
    "# Step 3: visualize weights of surrogate model, as we did for the original logistic regression model\n",
    "eli5.explain_weights(surrogate_model_rf[-1], feature_names=surrogate_model_rf[:-1].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4466b",
   "metadata": {},
   "source": [
    "Now that we have the weights of the surrogate model, what can we say about how the Random Forest model makes its predictions? What features seem more important? Are they similar to what we have seen for the other models so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e6991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f49878",
   "metadata": {},
   "source": [
    "**Note:** using a surrogate model is not always a very good strategy, because the simpler \"white box\" model is often unable to replicate the behavior of the most complex \"black box\" model. We can get a sense of how close the surrogate is approximating the original model by looking at the R<sup>2</sup> score. In the paper, they do so when trying to create a surrogate for XGBoost, and they explain: \n",
    "\n",
    "*The R<sup>2</sup> value between the XGBoost predictions and the surrogate model predictions on the test set is 0.38. The surrogate model only explains 38% of the variance in the XGBoost model’s predictions*\n",
    "\n",
    "Test this for the random forest surrogate model. How much variance is it able to capture?\n",
    "\n",
    "**Hints:**\n",
    "- Think carefully about what constitues the array of predictions and the array of ground truths in this case\n",
    "- You may remember that R<sup>2</sup> is, in fact, a metric for regression, not for classification! How can we use R<sup>2</sup> in this case? There are various ways to approximate R<sup>2</sup> for classification, as explained [here](https://datascience.oneoffcoder.com/psuedo-r-squared-logistic-regression.html). We will use the simplest one and use **count R<sup>2</sup>**, which is simply the accuracy of the surrogate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "surrogate_pred = surrogate_model_rf.predict(X_train)\n",
    "\n",
    "# compute count R2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66187d5d",
   "metadata": {},
   "source": [
    "Now, repeat the analysis through surrogate model for XGBoost. Comment on the results, including considerations on the following:\n",
    "- What seem to be the most important features?\n",
    "- How do the sets of most important features compare across models (do not forget logistic regression and decision tree in this comparison)?\n",
    "- How good are the surrogate models, in terms of capturing the variance of the original model? Are they reliable?\n",
    "- ...more thoughts of your choice..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018704b",
   "metadata": {},
   "source": [
    "#### Surrogate for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb2914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d49d08",
   "metadata": {},
   "source": [
    "### Question 12: Evaluation of Non-inherently Interpretable Models Using Permutation Feature Importance\n",
    "\n",
    "Another method used to interpret black box models is using feature permutation, which means changing the value of a feature and observing changes in the model's prediction error. More important features, when changed, will result in more frequent mistakes.\n",
    "\n",
    "Luckily for us, Permutation Feature Importance already exists as a function in Scikit-Learn! All you have to do it is looking at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) to learn how it works, and apply it to the 3 non-inherently interpretable models of this exercise. Let's start with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2319f8a",
   "metadata": {},
   "source": [
    "#### Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b4f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use permutation_importance on the random forest model, and save the result in a variable called \"out\"\n",
    "out = permutation_importance(rf_model, X_test, y_test, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5043522",
   "metadata": {},
   "source": [
    "After you are done, you can run the cell below to visualize the top 5 most important features in a bar chart. If you like, you can change the number of features shown or try other visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d095579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Name\": X_test.columns, \"Importance\": out[\"importances_mean\"], \"STD\": out[\"importances_std\"]})\n",
    "result = result.sort_values(by=['Importance'], ascending=False)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,7)})\n",
    "sns.barplot(data=result[:5], y=\"Name\", x=\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa9d82",
   "metadata": {},
   "source": [
    "Now, use Permutation Feature Importance on XGBoost.\n",
    "\n",
    "**Hint:** these are more complex models; if you find that this task is taking too long, you may consider reducing the number of permutations using the parameter `n_repeats`. Be aware that this produces more variable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251b843",
   "metadata": {},
   "source": [
    "#### XGBoost Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e60068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ee71e",
   "metadata": {},
   "source": [
    "Now that you have completed your analysis of feature importance using permutation, comment on the results. How do the sets of most important features compare with each other? Are this results similar to what you observed using the surrogate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabe44e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6adf64f",
   "metadata": {},
   "source": [
    "### Question 13: Evaluation of Non-inherently Interpretable Models Using SHAP\n",
    "\n",
    "The last method we are going to use to interpret the impact of each feature in our model is called SHAP, which stands for SHapley Additive exPlanations. How SHAP works is beyond the scope of this course, but if you are curious you can read the [original paper](https://arxiv.org/pdf/1705.07874.pdf) by Lundberg and Lee and check out [Lundberg's GitHub repo](https://github.com/shap/shap), which provides details on the implementation and examples.\n",
    "\n",
    "You will need to install SHAP to be able to use it:\n",
    "```\n",
    "pip install shap\n",
    "or\n",
    "conda install -c conda-forge shap\n",
    "```\n",
    "\n",
    "Then, import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2b9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap # may require to downgrade numpy to version = 1.23\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ed15e",
   "metadata": {},
   "source": [
    "SHAP needs the model (we will start with Random Forest) and samples to use to explain the predictions. For this, we will need to give it transformed samples (scaled and imputed, as required by the model) from  `X_train` or `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b84da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    data=rf_model.named_steps['ct'].transform(X_train),\n",
    "    columns=feature_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "\n",
    "X_test_enc = pd.DataFrame(\n",
    "    data=rf_model.named_steps['ct'].transform(X_test),\n",
    "    columns=feature_names,\n",
    "    index=X_test.index,\n",
    ")\n",
    "\n",
    "ind = np.random.choice(len(X_test_enc) - 1, 1000)  \n",
    "# This line just gives 1000 random indexes from the training set\n",
    "# We do this because getting SHAP values for all samples would be a bit too long, but you \n",
    "# are free to try it out!\n",
    "\n",
    "ind = np.append(ind, 5) # adding the hard sample - we'll need this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca462e0",
   "metadata": {},
   "source": [
    "The following lines are all that's needed to explain the model's predictions for a set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700995e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_explainer = shap.TreeExplainer(rf_model[-1])  # creating SHAP Explainer based on the model\n",
    "\n",
    "rf_shap_values = rf_explainer.shap_values(X_test_enc.iloc[ind])  # explaining predictions for 1000 random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd145e6e",
   "metadata": {},
   "source": [
    "This gives us the SHAP values for each sample and each feature (values for the positive class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50319f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_shap_values[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19468e",
   "metadata": {},
   "source": [
    "This is hardly interpretable, though. It is better to get the average values for each feature, which returns something similar to feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ca06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = np.abs(rf_shap_values[:,:,1]).mean(0)\n",
    "pd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8ce05",
   "metadata": {},
   "source": [
    "The SHAP library also has a lot of ways to visualize and interpret the SHAP values - try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437837c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap_figure = shap.summary_plot(rf_shap_values[:, :, 1], X_test_enc.iloc[ind], plot_size=[12,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239fb4c",
   "metadata": {},
   "source": [
    "Given the new information obtained using the SHAP library on the Random Forest model, explain the results (you will need to refer to the SHAP documentation - or ask us for help interpreting the plots) and comment on the difference between these results and those obtained using the other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515f7b7",
   "metadata": {},
   "source": [
    "\n",
    "Next, **repeat this analysis for XGBoost.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873368fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9b605",
   "metadata": {},
   "source": [
    "### Question 14: Explaining individual predictions using SHAP\n",
    "\n",
    "Another powerful feature of SHAP is that it allows us to explain the impact of each feature on individual predictions. For example, we will be able to explain how the prediction for our hard sample was generated. Let's start by looking at the prediction for this sample given by the random forest model. **Is it correct?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.predict(hard_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc330140",
   "metadata": {},
   "source": [
    "Let's look at the **force plot** for this particular prediction, by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7b7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    rf_explainer.expected_value[1],\n",
    "    rf_shap_values[:,:,1][-1],\n",
    "    X_test_enc.iloc[ind[-1]],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97854e5f",
   "metadata": {},
   "source": [
    "**Interpret the plot results,**, including the following:\n",
    "- What contributed the most to the prediction?\n",
    "- What countered the prediction the most?\n",
    "- Can we tell, by looking at the plot, that this was a difficult prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bed63",
   "metadata": {},
   "source": [
    "Finally, **repeat the analysis and comment on the results of the individual predictions made on the hard sample by XGBoost and Decision Tree** (since we were not able to do the latter earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f813a",
   "metadata": {},
   "source": [
    "# Part 5: Final Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3c2b9",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Using **all the results collected so far** on accuracy, fairness and transparency of the 5 model, write your recommendation about what model, in your opinion, should be employed for this application (300 words max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c12ede",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a71f4a6b",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "1) If you have completed this assignment in a group, please write a detailed description of how you divided the work and how you helped each other completing it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91004459",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a8fe51",
   "metadata": {},
   "source": [
    "2) Have you used ChatGPT or a similar Large Language Model (LLM) to complete this homework? Please describe how you used the tool. We will never deduct points for using LLMs for completing homework assignments, but this helps us understand how you are using the tool and advise you in case we believe you are using it incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012265f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92c08f2f",
   "metadata": {},
   "source": [
    "3) Have you struggled with some parts (or all) of this homework? Do you have pending questions you would like to ask? Write them down here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc52a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
